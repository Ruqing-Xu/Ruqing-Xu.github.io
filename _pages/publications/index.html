<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<!-- _pages/publications.md --><html><body>
<div class="publications">

<ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="xu2023delegation" class="col-sm-8">
        <!-- Title -->
        <div class="title">Persuasion, Delegation, and Private Information in Algorithm-Assisted Decisions</div>
        <!-- Author -->
        <div class="author">
        

        <em>Ruqing Xu</em>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Working paper</em>, 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2402.09384" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="/assets/pdf/delegation_xu.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>A principal designs an algorithm that generates a publicly observable prediction of a binary state. She must decide whether to act directly based on the prediction or to delegate the decision to an agent with private information but potential misalignment. We study the optimal design of the prediction algorithm and the delegation rule in such environments. Three key findings emerge: (1)  Delegation is optimal if and only if the principal would make the same binary decision as the agent had she observed the agent’s information. (2) Providing the most informative algorithm may be suboptimal even if the principal can act on the algorithm’s prediction. Instead, the optimal algorithm may provide more information about one state and restrict information about the other. (3) Common restrictions on algorithms, such as keeping a “human-in-the-loop” or requiring maximal prediction accuracy, strictly worsen decision quality in the absence of perfectly aligned agents and state-revealing signals. These findings predict the underperformance of human-machine collaborations if no measures are taken to mitigate common preference misalignment between algorithms and human decision-makers.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"></div>

        <!-- Entry bib key -->
        <div id="enem2023" class="col-sm-8">
        <!-- Title -->
        <div class="title">Stakes and Signals: An Empirical Investigation of Muddled Information in Standardized Testing</div>
        <!-- Author -->
        <div class="author">
        

        Germán Reyes*, Evan Riehl*, and <em>Ruqing Xu*</em>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Working paper</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="/assets/pdf/rrx_stakes_oct2023.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Muddled information models posit that higher stakes increase a signal’s informativeness about individuals’ <em>gaming ability</em> and decrease its informativeness for their <em>natural ability</em>. An important question is whether this muddling of abilities degrades the predictive value of a signal for long-run outcomes. We evaluate this question in the context of standardized testing by exploiting the introduction of Brazil’s national college admission exam, the ENEM. The staggered adoption of the ENEM by universities meant that, depending on their location and cohort, students either took a low-stakes school accountability test or a high-stakes test that governed admission to the most selective colleges in their area. Using ENEM records linked to nationwide college and labor market data, we find that the increase in the stakes of the ENEM exam made scores <em>more</em> informative for students’ longer-run outcomes. However, test score gaps between high- and lower-income students also expanded on the higher-stakes ENEM exam. Our results show that signals that include gaming ability can be more informative about individual productivity than signals that measure only natural ability, but they can also exacerbate socioeconomic inequality.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="xu2023decision" class="col-sm-8">
        <!-- Title -->
        <div class="title">Decision-aid or Controller? Steering Human Decision Makers with Algorithms</div>
        <!-- Author -->
        <div class="author">
        

        <em>Ruqing Xu</em>, and Sarah Dean</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>Working paper</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2303.13712" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="/assets/pdf/decision_paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="/assets/pdf/decision_presentation.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right"></span>
              <span class="__dimensions_badge_embed__" data-pmid="" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Algorithms are used to aid human decision makers by making predictions and recommending decisions. Currently, these algorithms are trained to optimize prediction accuracy. What if they were optimized to control final decisions? In this paper, we study a decision-aid algorithm that learns about the human decision maker and provides “personalized recommendations” to influence final decisions. We first consider fixed human decision functions which map observable features and the algorithm’s recommendations to final decisions. We characterize the conditions under which perfect control over final decisions is attainable. Under fairly general assumptions, the parameters of the human decision function can be identified from past interactions between the algorithm and the human decision maker, even when the algorithm was constrained to make truthful recommendations. We then consider a decision maker who is aware of the algorithm’s manipulation and responds strategically. By posing the setting as a variation of the cheap talk game [Crawford and Sobel, 1982], we show that all equilibria are partition equilibria where only coarse information is shared: the algorithm recommends an interval containing the ideal decision. We discuss the potential applications of such algorithms and their social implications.</p>
          </div>
        </div>
      </div>
</li>
</ol>

</div>
</body></html>
